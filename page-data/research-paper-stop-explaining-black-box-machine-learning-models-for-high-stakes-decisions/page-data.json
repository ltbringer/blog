{"componentChunkName":"component---src-templates-post-js","path":"/research-paper-stop-explaining-black-box-machine-learning-models-for-high-stakes-decisions","result":{"data":{"markdownRemark":{"html":"<div class=\"gatsby-highlight\" data-language=\"markdown\"><pre class=\"language-markdown\"><code class=\"language-markdown\">@misc{rudin2018stop,\n    title={Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},\n    author={Cynthia Rudin},\n    year={2018},\n    eprint={1811.10154},\n    archivePrefix={arXiv},\n    primaryClass={stat.ML}\n}</code></pre></div>\n<p>Key points:</p>\n<ol>\n<li>Explainable models.</li>\n<li>Interpretable models.</li>\n</ol>\n<h2 id=\"interpretable-models\" style=\"position:relative;\"><a href=\"#interpretable-models\" aria-label=\"interpretable models permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Interpretable Models</h2>\n<ul>\n<li>Constraints lie within the model.</li>\n<li>Obeys structural knowledge of the domain.</li>\n<li>Case based reasoning for edge-cases?</li>\n</ul>\n<h2 id=\"issues-with-explainable-ml\" style=\"position:relative;\"><a href=\"#issues-with-explainable-ml\" aria-label=\"issues with explainable ml permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Issues with explainable ML</h2>\n<h3 id=\"blackbox\" style=\"position:relative;\"><a href=\"#blackbox\" aria-label=\"blackbox permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Blackbox</h3>\n<ul>\n<li>Complicated for humans to understand.</li>\n<li>Enveloped around proprietary logic.</li>\n</ul>\n<h3 id=\"trade-off-between-accuracy-and-interpretability\" style=\"position:relative;\"><a href=\"#trade-off-between-accuracy-and-interpretability\" aria-label=\"trade off between accuracy and interpretability permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Trade-off between accuracy and interpretability.</h3>\n<ul>\n<li>If there is structure in data, with good representation of features. The difference in performance\nof complex classifiers (neural networks) against simpler classifiers (Logistic regression) is not high.</li>\n</ul>\n<h3 id=\"explainable-methods-provide-explanations-that-are-not-faithful-to-predictions-of-the-model\" style=\"position:relative;\"><a href=\"#explainable-methods-provide-explanations-that-are-not-faithful-to-predictions-of-the-model\" aria-label=\"explainable methods provide explanations that are not faithful to predictions of the model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Explainable methods provide explanations that are not faithful to predictions of the model.</h3>\n<blockquote>\n<p>If the explanation was completely faithful to what the original model computes, the explanation would equal the original model, and one would not need the original model in the first place ...</p>\n</blockquote>\n<h4 id=\"propublica-analysis\" style=\"position:relative;\"><a href=\"#propublica-analysis\" aria-label=\"propublica analysis permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ProPublica analysis</h4>\n<p>Accused COMPAS to be racially discriminative because the predictions match a model that uses racial information for parole and bail decisions. COMPAS, was conditioned on age and criminal history. The similarity in the behaviour shouldn't have been used as explanation.</p>\n<p>The authors claim that instead of using the term <code class=\"language-text\">explanation</code> if these are instead called <code class=\"language-text\">trends</code>, <code class=\"language-text\">summary statistcs</code> etc would be less misleading.</p>\n<h3 id=\"explanations-may-not-make-sense-or-provide-details\" style=\"position:relative;\"><a href=\"#explanations-may-not-make-sense-or-provide-details\" aria-label=\"explanations may not make sense or provide details permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Explanations may not make sense, or provide details.</h3>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 653px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/a0781209ad9ee016333648865a980695/e7dce/saliencymaps.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 30.05780346820809%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtUlEQVQY0x2Q30tTcRjG96942a04CqwuKoKKTLQM7CbbgmZrq7EiMu0XhBBkReDtvGhBhVcRgegJwVQq1ipB8pxtTXE77pyds50f286Yn75nLzzwXrx83ud5ApZl0Wg0aDab+LvXbtPy2lQNE8/z2N8Hx7FwbAN9r4ShqTRdi7pVx23YmKZGtaqKG1PIJuC6Lm0B8eVDa/U6uq6xvbNNpVLpPjEFfOdfic3NAkq+jFm1qdVqqOUS+S2ZvJJH0w1sWwB9d51ORwA9/Jl/l+bc8YOMhy5x41qY2dczzL1dpPd0iJFbSYZj15lOpdnI/qUvOMbhUzc5dvYJ50deUNrVCBSLRWRZJqcoqMJRavYVE/EIz54+ZupugkcT93j/+Q3xoSAvw0dIPujnYWqab3+2OBO+TGL8IqH7Vzl0YVKkUAk4jtO1b5pmt7v59BxTyRi3I1cYOBpkMh7lo/SBO6P9RMYG6AkOE409J5MrEE0MMjN0gMHRk/SeiFP2Hfq5/fJbrZaQRyGnsLy0wOqyxMoXSUTLIBd3kbLrSN+zLHxa5Xdmg/KeztqPr2R+LbG48lPcZjFE1/8BWLyCSLDaTtsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Fig: Saliency does not explain anything except where the network is looking.\"\n        title=\"Fig: Saliency does not explain anything except where the network is looking.\"\n        src=\"/blog/static/a0781209ad9ee016333648865a980695/e7dce/saliencymaps.png\"\n        srcset=\"/blog/static/a0781209ad9ee016333648865a980695/991de/saliencymaps.png 173w,\n/blog/static/a0781209ad9ee016333648865a980695/e4d6b/saliencymaps.png 345w,\n/blog/static/a0781209ad9ee016333648865a980695/e7dce/saliencymaps.png 653w\"\n        sizes=\"(max-width: 653px) 100vw, 653px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Fig: Saliency does not explain anything except where the network is looking.</figcaption>\n  </figure></p>\n<p>We have no idea why this image is labeled as either a dog or a musical instrument when considering only saliency. The explanations look essentially the same for both classes</p>\n<blockquote>\n<p>Poor explanations make it hard to troubleshoot a black box.</p>\n</blockquote>\n<h3 id=\"black-box-models-are-not-compatible-with-situations-where-information-outside-the-database-is-required-for-risk-assessment\" style=\"position:relative;\"><a href=\"#black-box-models-are-not-compatible-with-situations-where-information-outside-the-database-is-required-for-risk-assessment\" aria-label=\"black box models are not compatible with situations where information outside the database is required for risk assessment permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Black box models are not compatible with situations where information outside the database is required for risk-assessment.</h3>\n<p>The example case is from criminalogy, where circumstances of the crime are much worse than a generic assigned charge. This knowledge could increase or decrease <code class=\"language-text\">risk</code>. Black box models are hard to caliberate with such information. COMPAS model doesn't depend on the seriousness of the current crime.</p>\n<h3 id=\"black-box-models-with-explanations-can-lead-to-complicated-decisions\" style=\"position:relative;\"><a href=\"#black-box-models-with-explanations-can-lead-to-complicated-decisions\" aria-label=\"black box models with explanations can lead to complicated decisions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Black box models with explanations can lead to complicated decisions.</h3>\n<p>Typographical errors.</p>\n<h2 id=\"key-issues-with-interpretable-ml\" style=\"position:relative;\"><a href=\"#key-issues-with-interpretable-ml\" aria-label=\"key issues with interpretable ml permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Key issues with Interpretable ML</h2>\n<h3 id=\"corporations-make-profit-from-the-intellectual-property\" style=\"position:relative;\"><a href=\"#corporations-make-profit-from-the-intellectual-property\" aria-label=\"corporations make profit from the intellectual property permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Corporations make profit from the intellectual property.</h3>\n<h3 id=\"interpretable-models-can-entail-significant-effort-to-construct\" style=\"position:relative;\"><a href=\"#interpretable-models-can-entail-significant-effort-to-construct\" aria-label=\"interpretable models can entail significant effort to construct permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Interpretable models can entail significant effort to construct.</h3>\n<ul>\n<li>Compute</li>\n<li>Domain expertise</li>\n</ul>","timeToRead":2,"excerpt":"Key points: Explainable models. Interpretable models. Interpretable Models Constraints lie within the model. Obeys structural knowledge of…","frontmatter":{"title":"Research Paper: Stop Explaining Black Box Machine Learning Models for High Stakes Decisions...","cover":"https://unsplash.it/1152/300/?random?BirchintheRoses","date":"2020-05-08T00:00:00.000Z","categories":["readings"],"tags":["machine learning"]},"fields":{"slug":"/research-paper-stop-explaining-black-box-machine-learning-models-for-high-stakes-decisions","date":"May 07, 2020"}}},"pageContext":{"slug":"/research-paper-stop-explaining-black-box-machine-learning-models-for-high-stakes-decisions","nexttitle":"Taking Rust code to Python land","nextslug":"/taking-rust-code-to-python-land","prevtitle":"Hindley milner for humans","prevslug":"/hindley-milner-for-humans"}},"staticQueryHashes":["3969716136"]}